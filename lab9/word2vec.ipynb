{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anna/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "sourcefile = cwd + \"/sources/wiki.xml\"\n",
    "page_tag = \"{http://www.mediawiki.org/xml/export-0.10/}page\"\n",
    "text_tag= \"{http://www.mediawiki.org/xml/export-0.10/}text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse(sourcefile)\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts: 115541\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "\n",
    "counter = 0\n",
    "for elem in root.getiterator():\n",
    "    if(elem.tag == text_tag):\n",
    "        counter += 1\n",
    "        texts.append(elem.text)\n",
    "\n",
    "print('texts:', counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 500\n",
    "valid_articles = texts[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 2068.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences: 9841\n",
      "[['opengroup'], ['awk', 'czyta', 'wejście', 'linia', 'po', 'linii'], ['każda', 'linia', 'jest', 'przeszukiwana', 'pod', 'kątem', 'wzorców', 'występujących', 'w', 'programie', 'i', 'dla', 'każdego', 'pasującego', 'wzorca', 'wykonywana', 'jest', 'akcja', 'z', 'nim', 'skojarzona'], ['computerworld'], ['com'], ['wzorce', 'są', 'sprawdzane', 'w', 'kolejności', 'ich', 'pojawienia', 'się', 'w', 'programie'], ['domyślną', 'akcją', 'jest', 'wypisanie', 'rekordu'], ['awk', 'posiada', 'wbudowane', 'wsparcie', 'dla', 'wielu', 'funkcji'], ['np'], ['interpretacja', 'wartości', 'zmiennej', 'zależy', 'od', 'kontekstu']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for article in tqdm(valid_articles):\n",
    "    for sentence in article.split('.'):\n",
    "        if all(x.isalpha() or x.isspace() for x in sentence):\n",
    "            sentences.append([word.lower() for word in sentence.split() if len(word)>0])\n",
    "        \n",
    "print('sentences:', len(sentences))\n",
    "print(sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: 29484\n",
      "['opengroup', 'awk', 'czyta', 'wejście', 'linia', 'po', 'linii', 'każda', 'linia', 'jest']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for sentence in sentences:\n",
    "    for word in sentence:\n",
    "        words.append(word)\n",
    "        \n",
    "print('words:', len(words))\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words: 11522\n"
     ]
    }
   ],
   "source": [
    "words = set(words) # remove duplicates\n",
    "vocab_size = len(words) # number of unique words\n",
    "\n",
    "print('unique words:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2int = {}\n",
    "int2word = {}\n",
    "\n",
    "for i,word in enumerate(words):\n",
    "    word2int[word] = i\n",
    "    int2word[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "świadek\n"
     ]
    }
   ],
   "source": [
    "print(int2word[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3620\n"
     ]
    }
   ],
   "source": [
    "print(word2int['język'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9841/9841 [00:00<00:00, 70366.74it/s]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "WINDOW_SIZE = 2\n",
    "\n",
    "for sentence in tqdm(sentences):\n",
    "    for word_index, word in enumerate(sentence):\n",
    "        for nb_word in sentence[max(word_index - WINDOW_SIZE, 0) : min(word_index + WINDOW_SIZE, len(sentence)) + 1] :\n",
    "            if nb_word != word and nb_word:\n",
    "                data.append([word, nb_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['awk', 'czyta'], ['awk', 'wejście'], ['czyta', 'awk'], ['czyta', 'wejście'], ['czyta', 'linia'], ['wejście', 'awk'], ['wejście', 'czyta'], ['wejście', 'linia'], ['wejście', 'po'], ['linia', 'czyta']]\n"
     ]
    }
   ],
   "source": [
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5297"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2int['awk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 61420/73830 [00:09<00:02, 6173.39it/s]/Users/anna/anaconda3/lib/python3.6/site-packages/tqdm/_monitor.py:89: TqdmSynchronisationWarning: Set changed size during iteration (see https://github.com/tqdm/tqdm/issues/481)\n",
      "  TqdmSynchronisationWarning)\n",
      "100%|██████████| 73830/73830 [00:11<00:00, 6253.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# function to convert numbers to one hot vectors\n",
    "def to_one_hot(data_point_index, vocab_size):\n",
    "    temp = np.zeros(vocab_size)\n",
    "    temp[data_point_index] = 1\n",
    "    return temp\n",
    "\n",
    "x_train = [] # input word\n",
    "y_train = [] # output word\n",
    "\n",
    "for data_word in tqdm(data):\n",
    "    x_train.append(to_one_hot(word2int[ data_word[0] ], vocab_size))\n",
    "    y_train.append(to_one_hot(word2int[ data_word[1] ], vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# convert them to numpy arrays\n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73830, 11522) (73830, 11522)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making placeholders for x_train and y_train\n",
    "x = tf.placeholder(tf.float32, shape=(None, vocab_size))\n",
    "y_label = tf.placeholder(tf.float32, shape=(None, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 5 # you can choose your own number\n",
    "W1 = tf.Variable(tf.random_normal([vocab_size, EMBEDDING_DIM])) # weights\n",
    "b1 = tf.Variable(tf.random_normal([EMBEDDING_DIM])) # biases\n",
    "hidden_representation = tf.add(tf.matmul(x,W1), b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, vocab_size]))\n",
    "b2 = tf.Variable(tf.random_normal([vocab_size]))\n",
    "prediction = tf.nn.softmax(tf.add( tf.matmul(hidden_representation, W2), b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is :  16.865911\n",
      "loss is :  16.2043\n",
      "loss is :  15.628169\n",
      "loss is :  15.129018\n",
      "loss is :  14.699047\n",
      "loss is :  14.330756\n",
      "loss is :  14.017068\n",
      "loss is :  13.751227\n",
      "loss is :  13.526936\n",
      "loss is :  13.338434\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init) #make sure you do this!\n",
    "# define the loss function:\n",
    "cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), reduction_indices=[1]))\n",
    "# define the training step:\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy_loss)\n",
    "n_iters = 10\n",
    "# train for n_iter iterations\n",
    "for _ in range(n_iters):\n",
    "    sess.run(train_step, feed_dict={x: x_train, y_label: y_train})\n",
    "    print('loss is : ', sess.run(cross_entropy_loss, feed_dict={x: x_train, y_label: y_train}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.1337816e+00  2.3279121e+00  8.9919491e-04 -2.0784948e+00\n",
      "   5.1683038e-01]\n",
      " [ 8.4016436e-01  1.1743989e+00 -2.1808462e-02  1.2650520e-01\n",
      "   1.0124886e+00]\n",
      " [ 9.9099040e-01  7.2792476e-01  5.6595099e-01  3.4744936e-01\n",
      "   2.0102081e-01]\n",
      " ...\n",
      " [ 1.3402855e+00  1.1284367e+00  8.7614310e-01 -1.4204627e+00\n",
      "  -1.1074501e+00]\n",
      " [-9.9164270e-02  8.5978395e-01  4.3816090e-01  1.8480009e+00\n",
      "  -1.4754275e+00]\n",
      " [-4.1760433e-01 -1.4795698e-01 -1.1452730e+00 -3.9435998e-01\n",
      "  -1.1138235e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(W1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = sess.run(W1 + b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.24391964 -1.3446827   0.08939427 -1.0694506   1.6355885 ]\n"
     ]
    }
   ],
   "source": [
    "print(vectors[word2int['język'] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(vec1, vec2):\n",
    "    return np.sqrt(np.sum((vec1-vec2)**2))\n",
    "\n",
    "def find_closest(word_index, vectors):\n",
    "    min_dist = 10000 # to act like positive infinity\n",
    "    min_index = -1\n",
    "    query_vector = vectors[word_index]\n",
    "    for index, vector in enumerate(vectors):\n",
    "        if euclidean_dist(vector, query_vector) < min_dist and not np.array_equal(vector, query_vector):\n",
    "            min_dist = euclidean_dist(vector, query_vector)\n",
    "            min_index = index\n",
    "    return min_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "publikacja\n"
     ]
    }
   ],
   "source": [
    "print(int2word[find_closest(word2int['polska'], vectors)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "tree = spatial.KDTree(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wersją'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = vectors[word2int['mieć']]\n",
    "v2 = vectors[word2int['miał']]\n",
    "v3 = vectors[word2int['był']]\n",
    "int2word[tree.query(v1-v2+v3)[1]]\n",
    "# expected: być"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'news'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = vectors[word2int['ona']]\n",
    "v2 = vectors[word2int['jej']]\n",
    "v3 = vectors[word2int['jego']]\n",
    "int2word[tree.query(v1-v2+v3)[1]]\n",
    "# expected: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
