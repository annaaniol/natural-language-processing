{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dir = \"./sources/training/\"\n",
    "test_dir = \"./sources/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get raw text as string.\n",
    "with open(training_dir+\"A_A_Milne_-_Chatka_Puchatka.txt\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model.\n",
    "text_model = markovify.Text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_size: 2\n"
     ]
    }
   ],
   "source": [
    "model = text_model.to_dict()\n",
    "\n",
    "print('state_size:', model['state_size'])\n",
    "# print('chain:', model['chain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nie słyszałem ich dobrze i w chwilę potem okazało się, że on już przedtem wiedział.\n"
     ]
    }
   ],
   "source": [
    "# Print a randomly-generated sentence\n",
    "for i in range(1):\n",
    "    print(text_model.make_sentence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_A_Milne_\n",
      "['A_A_Milne_-_Chatka_Puchatka.txt', 'A_A_Milne_-_Kubus_Puchatek.txt']\n",
      "A_E_Van_Vogt_\n",
      "['A_E_Van_Vogt_-_Sklepy_z_bronia_na_Isher.txt']\n",
      "A_Imie_Jej_Ciemnosc\n",
      "['A_Imie_Jej_Ciemnosc.txt']\n",
      "A_S_LaVey_\n",
      "['A_S_LaVey_-_Biblia_Szatana.txt']\n",
      "A_i_B_Strugaccy_\n",
      "['A_i_B_Strugaccy_-_Miliard_lat_przed_koncem_swiata.txt', 'A_i_B_Strugaccy_-_Pora_deszczow.txt']\n",
      "Abe_Kobo_\n",
      "['Abe_Kobo_-_Czwarta_epoka.txt', 'Abe_Kobo_-_Kobieta_z_Wydm.txt']\n",
      "Adam_Bilikiewicz_\n",
      "['Adam_Bilikiewicz_-_Psychiatria.txt']\n",
      "Adam_Mickiewicz_\n",
      "['Adam_Mickiewicz_-_Pan_Tadeuszosloskop_net.txt']\n",
      "Adam_Wi?niewski_\n",
      "['Adam_Wi?niewski_-_Snerg_-_Wed?ug_?otra.txt']\n",
      "Adolf_Nowaczy?ski_\n",
      "['Adolf_Nowaczy?ski_-_Wielki_Fryderyk.txt']\n",
      "Agata_Christie_\n",
      "['Agata_Christie_-_Spotkanie_w_Bagdadzie.txt', 'Agata_Christie_-_Tajemnica_Wawrzynow.txt']\n",
      "Agent_Fundacji\n",
      "['Agent_Fundacji.txt']\n",
      "Ahern_Jerry_\n",
      "['Ahern_Jerry_-_Krucjata_3_-_Poszukiwanie.txt', 'Ahern_Jerry_-_Krucjata_5_-_Pajecza_siec.txt']\n",
      "Akta_odessy_Friderick_Forsyth\n",
      "['Akta_odessy_Friderick_Forsyth.txt']\n",
      "Albert_Camus_\n",
      "['Albert_Camus_-_Upadek.txt']\n",
      "Albert_Speer_\n",
      "['Albert_Speer_-_Wspomnienia.txt']\n",
      "Alberto_Moravia_\n",
      "['Alberto_Moravia_-_Rzymianka.txt']\n",
      "Alchemik\n",
      "['Alchemik.txt']\n",
      "Aldiss_Brian_W_\n",
      "['Aldiss_Brian_W_-_Lepsza_przemiana.txt', 'Aldiss_Brian_W_-_Potwory_z_Planety_Niewdziecznosci.txt', 'Aldiss_Brian_W_-_Swastyka!.txt']\n",
      "Aldous_Huxley_\n",
      "['Aldous_Huxley_-_Nowy_wspanialy_swiat.txt']\n",
      "Aldridge_Ray_\n",
      "['Aldridge_Ray_-_Blekitna_skora.txt', 'Aldridge_Ray_-_Frajerski_szmal.txt', 'Aldridge_Ray_-_Somanekiny.txt']\n",
      "Aleister_Crowley_\n",
      "['Aleister_Crowley_-_Joga.txt']\n",
      "Alejchem_Szolem_\n",
      "['Alejchem_Szolem_-_Notatki_komiwoja?era.txt']\n",
      "Aleksander_?wi?tochowski_\n",
      "['Aleksander_?wi?tochowski_-_Liberum_Veto.txt']\n",
      "Aleksander_Olin_\n",
      "['Aleksander_Olin_-_Komusutra.txt']\n",
      "Aleksander_Scibor\n",
      "['Aleksander_Scibor-Rylski_-_Czlowiek_z_marmuru.txt']\n",
      "Aleksander_Scibor_\n",
      "['Aleksander_Scibor_-_Rylski_-_Czlowiek_z_Zelaza.txt']\n",
      "Alex_Joe_\n",
      "['Alex_Joe_-_Powiem_wam_jak_zginal.txt']\n",
      "Alfred_Rambaud_\n",
      "['Alfred_Rambaud_-_Pier?cie?_Cezara.txt']\n",
      "Alfred_Szklarski_\n",
      "['Alfred_Szklarski_-_Tomek_u_zrodel_Amazonki.txt', 'Alfred_Szklarski_-_Tomek_w_Gran_Chaco.txt', 'Alfred_Szklarski_-_Tomek_w_Krainie_Kangurow.txt', 'Alfred_Szklarski_-_Tomek_wsrod_lowcow_glow.txt']\n",
      "Alistair_MacLean_\n",
      "['Alistair_MacLean_-_Athabaska.txt', 'Alistair_MacLean_-_HMS_Ulisses.txt', 'Alistair_MacLean_-_Mroczny_Krzyzowiec.txt', 'Alistair_MacLean_-_Przelecz_zlamanego_serca.txt', 'Alistair_MacLean_-_Stacja_arktyczna_Zebra.txt']\n",
      "Alistair_Maclean_\n",
      "['Alistair_Maclean_-_Pociag_Smierci.txt', 'Alistair_Maclean_-_Tabor.txt']\n",
      "Alistar_MacLean_\n",
      "['Alistar_MacLean_-_Zlote_rendez-vous.txt']\n"
     ]
    }
   ],
   "source": [
    "# Extract source files and authors\n",
    "\n",
    "files = sorted(os.listdir(training_dir))\n",
    "files = files[:50]\n",
    "authors = {}\n",
    "\n",
    "for s in files:\n",
    "    with open(s) as f:\n",
    "    text = f.read()\n",
    "    \n",
    "    no_extension = s[0:s.find('.')]\n",
    "    try:\n",
    "        index = s.index('-')\n",
    "    except ValueError:\n",
    "        index = len(s)\n",
    "    author = no_extension[:index]\n",
    "    assigned_files = authors.get(author, [])\n",
    "    assigned_files.append(s)\n",
    "    authors[author] = assigned_files\n",
    "    \n",
    "for k,v in authors.items():\n",
    "    print(k)\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = {}\n",
    "all_generated = []\n",
    "\n",
    "for author in authors:\n",
    "    files = authors[author]\n",
    "    \n",
    "    # Get raw text as string.\n",
    "    for file in files:\n",
    "        text = ''\n",
    "        with open(training_dir+file) as f:\n",
    "            text += f.read()\n",
    "            \n",
    "    # Build the model.\n",
    "    text_model = markovify.Text(text)\n",
    "    # print('\\n',author)\n",
    "    generated_sentences = ''\n",
    "        \n",
    "    # Print a randomly-generated sentence\n",
    "    for i in range(50):\n",
    "        sentence = str(text_model.make_sentence())\n",
    "        # print(sentence)\n",
    "        generated_sentences += sentence\n",
    "    # print(generated_sentences)\n",
    "        \n",
    "    generated[author] = generated_sentences\n",
    "    all_generated.append(generated_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, utils, similarities\n",
    "from gensim.models import LsiModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[w.lower() for w in word_tokenize(text) if w not in string.punctuation] for text in all_generated]\n",
    "# print(texts)\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save('/tmp/deerwester.dict')\n",
    "# print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting each document into the bag-of-words format\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('/tmp/deerwester.mm', corpus)\n",
    "\n",
    "corpus = corpora.MmCorpus('/tmp/deerwester.mm')\n",
    "id2word = corpora.Dictionary.load('/tmp/deerwester.dict')\n",
    "\n",
    "lsi = models.LsiModel(corpus, id2word=id2word, num_topics=100, chunksize=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_author = 'A_A_Milne'\n",
    "test_file = test_dir + test_author + '.txt'\n",
    "\n",
    "with open(test_file) as f:\n",
    "    test_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 22.92668060663625), (1, 2.6846355218427504), (2, -0.4921776099770541), (3, -3.4317637339707683), (4, 3.0032906594894553), (5, -0.5287377570973137), (6, -1.8406910233823326), (7, -3.1079086364510924), (8, 2.0705832998308047), (9, 3.747855918858471), (10, -0.5305512495477465), (11, -1.2739782466734555), (12, 0.04687543252580676), (13, -2.4689424092133407), (14, 1.5851102075308237), (15, -0.33006878918891003), (16, -1.0962394647311304), (17, -1.8388767719838248), (18, 0.22834254168586762), (19, -1.0497305065719387), (20, 0.2501080870627827), (21, -0.4883630118119687), (22, -0.36109437974817), (23, -0.6656961092351791), (24, -0.1365760484494392), (25, 1.10620706996828), (26, -0.6254683765917782), (27, 0.8239370876075615), (28, 0.5673094339495134), (29, -0.5098182776241748), (30, -1.4456922092706788), (31, -0.4749582534806645), (32, -0.6535086566183084)]\n",
      "[0, 9, 4, 1, 8, 14, 25, 27, 28, 20, 18, 12, 24, 15, 22, 31, 21, 2, 29, 5, 10, 26, 32, 23, 19, 16, 11, 30, 17, 6, 13, 7, 3]\n",
      "\n",
      "Matched author: A_A_Milne_\n"
     ]
    }
   ],
   "source": [
    "vec_bow = dictionary.doc2bow(test_text.lower().split())\n",
    "vec_lsi = lsi[vec_bow] # convert the query to LSI space\n",
    "print(vec_lsi)\n",
    "    \n",
    "# index = similarities.MatrixSimilarity(lsi[corpus])\n",
    "# index.save('/tmp/deerwester.index')\n",
    "# index = similarities.MatrixSimilarity.load('/tmp/deerwester.index')\n",
    "# sims = index[vec_lsi] # perform a similarity query against the corpus\n",
    "# result = list(enumerate(sims))\n",
    "# print(result)\n",
    "# top_per_doc = sorted(result, key=lambda x: x[1], reverse=True)\n",
    "# top_per_doc = list(item[0] for item in top_per_doc)\n",
    "\n",
    "top_per_doc = sorted(vec_lsi, key=lambda x: x[1], reverse=True)\n",
    "top_per_doc = list(item[0] for item in top_per_doc)\n",
    "\n",
    "print(top_per_doc)\n",
    "\n",
    "best_match = list(generated.keys())[top_per_doc[0]]\n",
    "print(\"\\nMatched author:\",best_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(test_author):\n",
    "    test_file = test_dir + test_author + '.txt'\n",
    "\n",
    "    with open(test_file) as f:\n",
    "        test_text = f.read()\n",
    "        \n",
    "    vec_bow = dictionary.doc2bow(test_text.lower().split())\n",
    "    vec_lsi = lsi[vec_bow] # convert the query to LSI space\n",
    "    print(vec_lsi)\n",
    "    \n",
    "    top_per_doc = sorted(vec_lsi, key=lambda x: x[1], reverse=True)\n",
    "    top_per_doc = list(item[0] for item in top_per_doc)\n",
    "\n",
    "    print(top_per_doc)\n",
    "\n",
    "    best_match = list(generated.keys())[top_per_doc[0]]\n",
    "    print(\"\\nMatched author:\",best_match)\n",
    "    return best_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 22.92668060663625), (1, 2.6846355218427504), (2, -0.4921776099770541), (3, -3.4317637339707683), (4, 3.0032906594894553), (5, -0.5287377570973137), (6, -1.8406910233823326), (7, -3.1079086364510924), (8, 2.0705832998308047), (9, 3.747855918858471), (10, -0.5305512495477465), (11, -1.2739782466734555), (12, 0.04687543252580676), (13, -2.4689424092133407), (14, 1.5851102075308237), (15, -0.33006878918891003), (16, -1.0962394647311304), (17, -1.8388767719838248), (18, 0.22834254168586762), (19, -1.0497305065719387), (20, 0.2501080870627827), (21, -0.4883630118119687), (22, -0.36109437974817), (23, -0.6656961092351791), (24, -0.1365760484494392), (25, 1.10620706996828), (26, -0.6254683765917782), (27, 0.8239370876075615), (28, 0.5673094339495134), (29, -0.5098182776241748), (30, -1.4456922092706788), (31, -0.4749582534806645), (32, -0.6535086566183084)]\n",
      "[0, 9, 4, 1, 8, 14, 25, 27, 28, 20, 18, 12, 24, 15, 22, 31, 21, 2, 29, 5, 10, 26, 32, 23, 19, 16, 11, 30, 17, 6, 13, 7, 3]\n",
      "\n",
      "Matched author: A_A_Milne_\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A_A_Milne_'"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment('A_A_Milne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
