{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NullHandler', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_matutils', 'corpora', 'interfaces', 'logger', 'logging', 'matutils', 'models', 'parsing', 'scripts', 'similarities', 'summarization', 'topic_coherence', 'utils']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "print(dir(gensim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "sourcefile = cwd + \"/sources/pap.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 51574\n"
     ]
    }
   ],
   "source": [
    "raw_documents = []\n",
    "\n",
    "with open(sourcefile) as f: \n",
    "    contents = f.read()\n",
    "    for note in contents.split(\"#\"):\n",
    "            if len(note) > 0:\n",
    "                raw_documents.append(note)\n",
    "                \n",
    "print(\"Number of documents:\",len(raw_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "gen_docs = [[w.lower() for w in word_tokenize(text) if w not in string.punctuation] \n",
    "            for text in raw_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in dictionary: 223500\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "dictionary.filter_n_most_frequent(2)\n",
    "print(\"Number of words in dictionary:\", len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TF-IDF PART ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = gensim.models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_tfidf(doc):\n",
    "    query_doc = [w.lower() for w in doc]\n",
    "    query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "    query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "\n",
    "    sim = gensim.similarities.MatrixSimilarity(corpus) \n",
    "    return sim[query_doc_tf_idf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_top_n_tfidf(n, doc_id):\n",
    "    doc_index = doc_id - 1\n",
    "    top_n = []\n",
    "    sim = get_sim_tfidf(gen_docs[doc_index])\n",
    "    top_per_doc = sim.argsort()[-n:][::-1]\n",
    "    top_per_doc = [num + 1 for num in top_per_doc]\n",
    "    return top_per_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRAPH PART ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_corpus = []\n",
    "for i in range(0,10):\n",
    "    graph_corpus.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordsfile = cwd + \"/sources/stopwords.txt\"\n",
    "stop_words = []\n",
    "\n",
    "with open(stopwordsfile) as f: \n",
    "    contents = f.read()\n",
    "    for word in contents.split(\", \"):\n",
    "            if len(word) > 0:\n",
    "                stop_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "filtered_dictionary = copy.deepcopy(dictionary)\n",
    "\n",
    "for stop_word in stop_words:\n",
    "    if stop_word in filtered_dictionary.token2id:\n",
    "        filtered_dictionary.filter_tokens(bad_ids=[filtered_dictionary.token2id[stop_word]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zeros():\n",
    "    zeros = []\n",
    "    size = len(filtered_dictionary)\n",
    "    for i in range (0, size):\n",
    "        row = []\n",
    "        for r in range (0, size):\n",
    "            row.append(0)\n",
    "        zeros.append(row)\n",
    "    return zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_matrix(doc, k):\n",
    "    doc_matrix = get_zeros()\n",
    "    \n",
    "    query_doc = [w.lower() for w in doc]    \n",
    "    dict_size = len(filtered_dictionary)\n",
    "    doc_size = len(query_doc)\n",
    "    \n",
    "    for i in range(0, doc_size):\n",
    "        for slide in range(0, k+1):\n",
    "            if i+slide < doc_size and query_doc[i] in filtered_dictionary.token2id and query_doc[i+slide] in filtered_dictionary.token2id:\n",
    "                first = filtered_dictionary.token2id[query_doc[i]]\n",
    "                second = filtered_dictionary.token2id[query_doc[i+slide]]\n",
    "                doc_matrix[first][second] += 1\n",
    "    return doc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_vector(doc, k):\n",
    "    doc_matrix = get_doc_matrix(doc, k)\n",
    "    doc_vector = []\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    dict_size = len(filtered_dictionary)\n",
    "    \n",
    "    for i in range(0, dict_size):\n",
    "        for j in range(0, dict_size):\n",
    "            if doc_matrix[i][j] > 0:\n",
    "                doc_vector.append((counter, doc_matrix[i][j]))\n",
    "            counter += 1\n",
    "    return doc_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_corpus(k):\n",
    "    if graph_corpus[k] != 0:\n",
    "        return graph_corpus[k]\n",
    "    else:\n",
    "        doc_count = len(gen_docs)\n",
    "        corpus = []\n",
    "        for i in range(0, doc_count):\n",
    "            v = get_doc_vector(gen_docs[i], k)\n",
    "            corpus.append(v)\n",
    "        graph_corpus[k] = corpus\n",
    "        return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_graph(doc, k):\n",
    "    corpus = get_graph_corpus(k)\n",
    "    doc_vector = get_doc_vector(doc, k)\n",
    "    sim = gensim.similarities.MatrixSimilarity(corpus)\n",
    "    return sim[doc_vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_graph(n, doc_id, k):\n",
    "    doc_index = doc_id - 1\n",
    "    top_n = []\n",
    "    sim = get_sim_graph(gen_docs[doc_index], k)\n",
    "    top_per_doc = sim.argsort()[-n:][::-1]\n",
    "    top_per_doc = [num + 1 for num in top_per_doc]\n",
    "    return top_per_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXPERIMENT ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(note_id, k_list):\n",
    "    print('base note:', note_id)\n",
    "    print('tf-idf: ',get_top_n_tfidf(10, note_id))\n",
    "    for k in k_list:\n",
    "        print(k,'graph:',get_top_n_graph(10, note_id, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base note: 121\n",
      "tf-idf:  [121, 239, 8312, 26439, 2669, 42838, 3435, 7812, 2173, 17666]\n"
     ]
    }
   ],
   "source": [
    "experiment(121,[1,2,3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
