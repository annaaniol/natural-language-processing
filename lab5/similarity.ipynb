{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NullHandler', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_matutils', 'corpora', 'interfaces', 'logger', 'logging', 'matutils', 'models', 'parsing', 'scripts', 'similarities', 'summarization', 'topic_coherence', 'utils']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "print(dir(gensim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "sourcefile = cwd + \"/sources/pap.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 51574\n"
     ]
    }
   ],
   "source": [
    "raw_documents = []\n",
    "\n",
    "with open(sourcefile) as f: \n",
    "    contents = f.read()\n",
    "    for note in contents.split(\"#\"):\n",
    "            if len(note) > 0:\n",
    "                raw_documents.append(note)\n",
    "                \n",
    "print(\"Number of documents:\",len(raw_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "gen_docs = [[w.lower() for w in word_tokenize(text) if w not in string.punctuation] \n",
    "            for text in raw_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in dictionary: 223502\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "print(\"Number of words in dictionary:\", len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TF-IDF PART ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = gensim.models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_tfidf(doc):\n",
    "    query_doc = [w.lower() for w in doc]\n",
    "    query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "    query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "\n",
    "    sim = gensim.similarities.MatrixSimilarity(corpus) \n",
    "    return sim[query_doc_tf_idf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_top_n_tfidf(n, doc_id):\n",
    "    doc_index = doc_id - 1\n",
    "    sim = get_sim_tfidf(gen_docs[doc_index])\n",
    "    top_per_doc = sim.argsort()[-n:][::-1]\n",
    "    top_per_doc = [num + 1 for num in top_per_doc]\n",
    "    return top_per_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRAPH PART ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_corpus = []\n",
    "for i in range(0,10):\n",
    "    graph_corpus.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordsfile = cwd + \"/sources/stopwords.txt\"\n",
    "stop_words = []\n",
    "\n",
    "with open(stopwordsfile) as f: \n",
    "    contents = f.read()\n",
    "    for word in contents.split(\", \"):\n",
    "            if len(word) > 0:\n",
    "                stop_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "filtered_dictionary = copy.deepcopy(dictionary)\n",
    "\n",
    "for stop_word in stop_words:\n",
    "    if stop_word in filtered_dictionary.token2id:\n",
    "        filtered_dictionary.filter_tokens(bad_ids=[filtered_dictionary.token2id[stop_word]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zeros():\n",
    "    zeros = []\n",
    "    size = len(filtered_dictionary)\n",
    "    for i in range (0, size):\n",
    "        row = []\n",
    "        for r in range (0, size):\n",
    "            row.append(0)\n",
    "        zeros.append(row)\n",
    "    return zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_matrix(doc, k):\n",
    "    doc_matrix = get_zeros()\n",
    "    \n",
    "    query_doc = [w.lower() for w in doc]    \n",
    "    dict_size = len(filtered_dictionary)\n",
    "    doc_size = len(query_doc)\n",
    "    \n",
    "    for i in range(0, doc_size):\n",
    "        for slide in range(0, k+1):\n",
    "            if i+slide < doc_size and query_doc[i] in filtered_dictionary.token2id and query_doc[i+slide] in filtered_dictionary.token2id:\n",
    "                first = filtered_dictionary.token2id[query_doc[i]]\n",
    "                second = filtered_dictionary.token2id[query_doc[i+slide]]\n",
    "                doc_matrix[first][second] += 1\n",
    "    return doc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_vector(doc, k):\n",
    "    doc_matrix = get_doc_matrix(doc, k)\n",
    "    doc_vector = []\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    dict_size = len(filtered_dictionary)\n",
    "    \n",
    "    for i in range(0, dict_size):\n",
    "        for j in range(0, dict_size):\n",
    "            if doc_matrix[i][j] > 0:\n",
    "                doc_vector.append((counter, doc_matrix[i][j]))\n",
    "            counter += 1\n",
    "    return doc_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_corpus(k):\n",
    "    if graph_corpus[k] != 0:\n",
    "        return graph_corpus[k]\n",
    "    else:\n",
    "        doc_count = len(gen_docs)\n",
    "        corpus = []\n",
    "        for i in range(0, doc_count):\n",
    "            v = get_doc_vector(gen_docs[i], k)\n",
    "            corpus.append(v)\n",
    "        graph_corpus[k] = corpus\n",
    "        return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_graph(doc, k):\n",
    "    corpus = get_graph_corpus(k)\n",
    "    doc_vector = get_doc_vector(doc, k)\n",
    "    sim = gensim.similarities.MatrixSimilarity(corpus)\n",
    "    return sim[doc_vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_graph(n, doc_id, k):\n",
    "    doc_index = doc_id - 1\n",
    "    sim = get_sim_graph(gen_docs[doc_index], k)\n",
    "    top_per_doc = sim.argsort()[-n:][::-1]\n",
    "    top_per_doc = [num + 1 for num in top_per_doc]\n",
    "    return top_per_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ALTERNATIVE GRAPH PART ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_flat = []\n",
    "doc_count = len(gen_docs)\n",
    "for i in range(0,doc_count):\n",
    "    matrix_flat.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def get_top_n_graph_sparsed(n, doc_id, k):\n",
    "    doc_index = doc_id - 1\n",
    "    \n",
    "    doc_matrix = np.array([get_doc_matrix(gen_docs[doc_index], k)], dtype = np.int64)\n",
    "    doc_flat_matrix = np.hstack(doc_matrix)\n",
    "    doc_flat_matrix = np.hstack(doc_flat_matrix)\n",
    "    \n",
    "    doc_count = len(gen_docs)\n",
    "    dist_to_note = {}\n",
    "    for i in range(0, doc_count):\n",
    "        if len(matrix_flat[i]) != 0:\n",
    "            m_flat = matrix_flat[i]\n",
    "        else:\n",
    "            m = np.array([get_doc_matrix(gen_docs[i], k)], dtype = np.int64)\n",
    "            m_flat = np.hstack(m)\n",
    "            m_flat = np.hstack(m_flat)\n",
    "            matrix_flat[i] = m_flat\n",
    "            \n",
    "        dist = distance.cosine(doc_flat_matrix, m_flat)\n",
    "        dist_to_note[dist] = i+1\n",
    "    \n",
    "    n_items = dist_to_note\n",
    "    \n",
    "    top_per_doc  = dict(sorted(dist_to_note.items(), key=lambda x: x[0], reverse=False)[:n])\n",
    "    top_per_doc = list(top_per_doc.values())\n",
    "    return top_per_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXPERIMENT ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(note_id, k_list, n):\n",
    "    print('base note:', note_id)\n",
    "    print('tf-idf: ',get_top_n_tfidf(n, note_id))\n",
    "    for k in k_list:\n",
    "        print(k,'graph:',get_top_n_graph_sparsed(n, note_id, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base note: 121\n",
      "tf-idf:  [121, 8312, 239, 17666, 1199, 7812, 2173, 373, 26133, 10411]\n"
     ]
    }
   ],
   "source": [
    "experiment(121,[1,2,3],10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
